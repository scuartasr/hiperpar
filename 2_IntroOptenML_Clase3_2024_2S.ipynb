{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVLYVoQvQ0Rm"
      },
      "source": [
        "<table>\n",
        "    <tr>\n",
        "        <td style=\"text-align:left\">\n",
        "            <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR9ItLTT_F-3Q30cu7ZCCoKmuFGBt22pe7pNA\" alt=\"Logo Universidad\" width=\"300\"/>\n",
        "        </td>\n",
        "        <td>\n",
        "             Departamento de Ciencias de la Computación y de la Decisión<br>\n",
        "            Facultad de Minas<br>\n",
        "            Universidad Nacional de Colombia<br>\n",
        "            Optimizacion e IA 2024-2S<br><br>\n",
        "            Docente: Maria Constanza Torres Madronero<br>\n",
        "            <br>\n",
        "            Contribuciones a la guia por: <br>\n",
        "            - Deimer Miranda Montoya (2023)<br>\n",
        "            - Luis Fernando Becerra Monsalve (2024)\n",
        "        </td>    \n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz6DFPE8Q0Rn"
      },
      "source": [
        "# **Clasificacion supervisada**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5otWvp5hQ0Ro"
      },
      "source": [
        "\n",
        "\n",
        "#### (a) Preparación de los datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekaMEqDsQ0Ro"
      },
      "source": [
        "Scikit-learn, también conocida como sklearn, es una librería gratuita de aprendizaje de máquina para Python. Desarrollada inicialmente por David Cournapeau en Google Summer en 2007. Incluye diferentes métodos de clasificación, regresión, y clustering incluyendo máquinas de soporte vectorial (SVM), bosques aleatorios (RF), K-means y DBSCAN.  \n",
        "\n",
        "Toda la documentación sobre esta librería puede ser consultada en el siguiente [aquí.](https://scikit-learn.org/stable/ )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LDp4bA8GQ0Ro"
      },
      "outputs": [],
      "source": [
        "#La libreria scikit-learn - incluida en colab - incluye\n",
        "#varios datasets para aprender a usar diferentes algoritmos de aprendizaje\n",
        "#de maquinas. Vamos a usar en esta clase el dataset Iris\n",
        "\n",
        "#Iris: incluye 150 muestras (50 por clase) con 4 atributos\n",
        "#Los datos incluidos en sklearn ya se encuentran organizados y limpios\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Las clases incluidas en Iris corresponden a tres tipos de lirios\n",
        "\n",
        "\n",
        "#Las caracteristicas estan asociadas al largo y ancho de los petalos y el\n",
        "#sépalo\n",
        "\n"
      ],
      "metadata": {
        "id": "rOXN6C5Lg6Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos a revisar los primeros 5 elementos del conjunto de datos\n",
        "\n",
        "\n",
        "#y revisemos las etiquetas (0:setosa, 1:versicolor, 2:virginica)\n"
      ],
      "metadata": {
        "id": "xLG4yHYuhEjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la manipulación de los datos vamos a emplear los DataFrames de Pandas.\n",
        "Pandas es una de las principales librerías de Python. Las estructuras más utilizadas en esta librería son las Series y los DataFrames. Las Series son objetos unidimensionales y los DataFrames son paneles bidimensionales compuestos por filas y columnas. El formato de los DataFrames puede compararse con los diccionarios de Python: las claves son los nombres de las columnas y los valores son las Series. Su estructura puede considerarse similar a la de una hoja de cálculo de Excel.\n",
        "\n",
        "Mayor información sobre la librería Pandas y uso de los DataFrame puede encontrarse en el libro de referencia del curso.\n"
      ],
      "metadata": {
        "id": "QStslwsN_l46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Primero, importamos la libreria\n",
        "\n",
        "\n",
        "#Organizamos los datos en un DataFrame\n"
      ],
      "metadata": {
        "id": "gsOmS84MimTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos a dividir los datos entre las muestras de entrenamiento y prueba\n",
        "\n",
        "#Muestras\n",
        "\n",
        "#Etiquetas\n",
        "\n",
        "#Seleccionamos 70% de muestras para entrenamiento y 30% para prueba\n"
      ],
      "metadata": {
        "id": "iDq3iGh_jeUW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (b) Random Forest\n",
        "\n",
        "Random Forest (Bosque Aleatorio) es un algoritmo de aprendizaje de máquina, introducido por Leo Breiman y Adele Cutler, que combina los resultados de varios árboles de decisión para llegar a un único resultado.\n",
        "\n",
        "*   Para conocer más sobre arboles de decisión visita https://www.ibm.com/topics/decision-trees\n",
        "*   En este enlace mayor documentación sobre Random ForestÑ https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
        "\n"
      ],
      "metadata": {
        "id": "2cgciCqHBDMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos el modelo de bosques aleatorios para su entrenamiento\n",
        "\n",
        "\n",
        "#Entrenamos el modelo con parametros fijos\n",
        "\n",
        "\n",
        "#Una vez entrenado el modelo, lo aplicamos a los datos de prueba\n"
      ],
      "metadata": {
        "id": "xwLQR5VmkKqF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos a calcular las metricas de precision para evaluar el desempeño del clasificador\n",
        "\n",
        "#Por ejemplo, la precisión se calcula a partir del numero total de muestras\n",
        "#correctamente clasificadas sobre el numero total de muestras\n",
        "\n"
      ],
      "metadata": {
        "id": "oPBzBOo2k5Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#podemos tambien analizar la matriz de confusion\n"
      ],
      "metadata": {
        "id": "C3AZ-ah4wDio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Podemos visualizar los arboles de decisión entrenados en el proceso\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "for i in range(3):\n",
        "    tree = RFclas.estimators_[i]\n",
        "    dot_data = export_graphviz(tree,\n",
        "                               feature_names=X_train.columns,\n",
        "                               filled=True,\n",
        "                               max_depth=2,\n",
        "                               impurity=False,\n",
        "                               proportion=True)\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    display(graph)"
      ],
      "metadata": {
        "id": "9ZQ8jcDNofzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(c) Parametros de Random Forest  \n",
        "1. Parámetros de la bolsa (Bagging)\n",
        "\n",
        "*   n_estimators: Numero de árboles de decisión\n",
        "*   boostrap: uso de muestras con o sin remplazo\n",
        "*   criterion: evaluación de la división del nodo\n",
        "\n",
        "2. parámetros de los arboles\n",
        "\n",
        "*   max_features: máximo número de características que pueden ser referenciadas en un partición\n",
        "*   max_depth: máxima profundidad del árbol de decisión\n",
        "*   min_samples_split: mínimo número de muestras para la subdivisión de un nodo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Os0TMdUVKZUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Veamos que ocurre, si cambiamos alguno de los parametros del modelo\n",
        "\n",
        "#Entrenamos\n",
        "\n",
        "#Aplicamos el modelo\n",
        "\n",
        "#Analizamos la precision:\n"
      ],
      "metadata": {
        "id": "-1Whfehmlb0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### (d) Seleccion de caracteristicas importantes\n",
        "Una de las herramientas de proporciona Random Forest es la medida de la importancia de las características. Es decir, que además de proporcionarnos un modelo de clasificación, es un selector de características. La importancia se determina por la frecuencia que una característica es seleccionada para la construcción de un árbol de decisión."
      ],
      "metadata": {
        "id": "04LVQMv5poJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#La informacion sobre la importancia de cada caracteristica se encuentra en el atributo\n",
        "#feature_importances_ del modelo.\n",
        "#Lo organizamos en una Serie para su visualizacion\n"
      ],
      "metadata": {
        "id": "NejqhGW1vDjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tambien podemos graficarlo\n",
        "\n",
        "\n",
        "#Este tipo de analisis permite decidir si queremos continuar con todo el conjunto\n",
        "#de caracteristicas, o seleccionar solo las mas importantes\n",
        "#Reducir el espacio de representacion permite reducir los costos computacionales\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GisWeVxwvZux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento de un modelo usando solo las caracteristicas mas relevantes\n",
        "#Seleccionamos el conjunto de caracteristicas\n",
        "X = data[['petal width', 'petal length', 'sepal length']]\n",
        "y = data['species']\n",
        "\n",
        "#Dividimos de nuevo el conjunto de datos\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
      ],
      "metadata": {
        "id": "qO8NWQ0Kv35k"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamos el clasificador\n",
        "RFclas3 = RandomForestClassifier(n_estimators=200, criterion=\"gini\", max_depth=5, random_state=0)\n",
        "RFclas3.fit(X_train,y_train)\n",
        "#Clasificamos\n",
        "y_pred = RFclas3.predict(X_test)\n",
        "#Calculamos la precision\n",
        "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))\n"
      ],
      "metadata": {
        "id": "KJF9cPrtwE-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Que ocurre si solo usamos dos caracteristicas?\n",
        "\n"
      ],
      "metadata": {
        "id": "MK55AK44xdBb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Anotaciones importantes\n",
        "1. Para el uso de Random Forest tenemos varios elementos que debemos definir de nuestro modelo\n",
        "2. Por un lado, tenemos que seleccionar el número de características de los datos con las que queremos trabajar. Una forma de hacerlo es, entrenando un modelo inicial de RF y usar la importancia de las características calculada por el algoritmo para solo quedarnos con las mas relevantes. Otra alternativa es aplicar otros métodos de selección de características.\n",
        "3. Por otro lado, debemos definir los hiperparámetros del clasificador. En la literatura, se encuentran muchos resultados que ofrecen una idea de que valores utilizar con datasets como Iris. Sin embargo, para casos reales es necesario realizar una selección adecuada de los hiperparámetros.\n"
      ],
      "metadata": {
        "id": "RLKY7cOTY5pk"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}